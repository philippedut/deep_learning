{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b494e3",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In this notebook we make the final ruun of the preoject the idea is to aggregate all the model build for each phylum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd39aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
    "extract_path = '/content/rare_species 1'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Rescaling, RandAugment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# With colab\n",
    "folder_path = '/content/rare_species 1/rare_species 1'\n",
    "meta = pd.read_csv('/content/rare_species 1/rare_species 1/metadata.csv')\n",
    "\n",
    "\n",
    "# With vscode\n",
    "# folder_path = '../data/rare_species 1'\n",
    "# meta = pd.read_csv('../data/rare_species 1/metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854840f",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460b785",
   "metadata": {},
   "source": [
    "## Creating a test set\n",
    "\n",
    "In order for the to by able to have a test set that is not split by phylum we need to do it before spliting each images into a folder for their specific Phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_val , meta_test = train_test_split(meta, test_size=0.1, stratify=meta['category'], random_state=42, stratify =meta['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1734b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "current_locations = '/content/rare_species 1/rare_species 1'\n",
    "\n",
    "# with vscode\n",
    "# current_locations = '../data/rare_species 1'\n",
    "\n",
    "for _, row in meta_train_val.iterrows():\n",
    "\n",
    "    phylum = row['phylum']\n",
    "    file_path = row['file_path']\n",
    "\n",
    "\n",
    "    file_location = os.path.join(current_locations, file_path)\n",
    "\n",
    "    # create a a detination folder keeping the subfolder structure\n",
    "\n",
    "        # with colab\n",
    "    target_folder = os.path.join(phylum, os.path.dirname(file_path))\n",
    "\n",
    "        # with vscode\n",
    "    # target_folder = os.path.join(\"../data\" , phylum, os.path.dirname(file_path))\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "    # Final destination path\n",
    "    destination = os.path.join(target_folder, os.path.basename(file_path))\n",
    "\n",
    "    # Copy the file if it exists\n",
    "    if os.path.exists(file_location):\n",
    "        shutil.copy2(file_location, destination)\n",
    "    else:\n",
    "        print(f\"Couldn't find the file: {file_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3be99",
   "metadata": {},
   "source": [
    "# Final Train, Val, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with colab\n",
    "path_phylum_athropoda = \"/content/arthropoda\"\n",
    "path_phylum_chordata = \"/content/chordata\"\n",
    "path_phylum_cnidaria = \"/content/cnidaria\"\n",
    "path_phylum_mollusca = \"/content/mollusca\"\n",
    "\n",
    "# with vscode\n",
    "# path_phylum_athropoda = \"../data/arthropoda\"\n",
    "# path_phylum_chordata = \"../data/chordata\"\n",
    "# path_phylum_cnidaria = \"../data/cnidaria\"\n",
    "# path_phylum_mollusca = \"../data/mollusca\"\n",
    "\n",
    "image_size = (224, 224)\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "train_ds_athropoda, val_athropoda= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_athropoda,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_chordata, val_chordata= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_chordata,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_cnidaria, val_cnidaria= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_cnidaria,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_mollusca, val_mollusca= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_mollusca,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(train_ds):\n",
    "    for images, labels in train_ds.take(1):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "            plt.title(int(labels[i]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(train_ds_athropoda)\n",
    "display_images(train_ds_chordata)\n",
    "display_images(train_ds_cnidaria)\n",
    "display_images(train_ds_mollusca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818d203",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(images, augmentation_layers):\n",
    "    for layer in augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc238da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_athropoda= [\n",
    " ## add any you want here\n",
    "]\n",
    "\n",
    "data_augmentation_chordata= keras.Sequential([\n",
    "\n",
    "    # apply any kind of kera preprocessing randomly\n",
    "    layers.RandAugment(value_range=(0, 255), num_ops=2),\n",
    "\n",
    "    # change the image by moving or zooming in\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2), # 20 percent rotation\n",
    "    layers.RandomZoom(0.2), # 20 percent rotation\n",
    "\n",
    "\n",
    "    # change the image rgbs --> contrast and brightness\n",
    "    layers.RandomContrast(0.2, value_range=(0, 255)), # change by 20%\n",
    "    layers.RandomBrightness(0.2, (0, 255)), # cahnge by 20 %\n",
    "\n",
    "    # adds noise to the images to prevent overfitting (blurry filter)\n",
    "    layers.GaussianNoise(0.1),\n",
    "\n",
    "])\n",
    "\n",
    "data_augmentation_cnidaria= [\n",
    "\n",
    " ## add any you want here\n",
    "]\n",
    "\n",
    "data_augmentation_mollusca = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),   # Rotate images randomly up to 20%\n",
    "    layers.RandomZoom(0.2),        # Zoom in/out randomly up to 20%\n",
    "    layers.RandomContrast(0.2)     # Change contrast randomly up to 20%\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e88b69",
   "metadata": {},
   "source": [
    "# Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_athropoda(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_chordata(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = data_augmentation_chordata(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained MobileNetV2 base\n",
    "    base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x) # to avoid over fitting\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.001))(x) #try to prevent overfitting\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.base_model = base_model # save thee base model to be able to call it back when fine tunning\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_cnidaria(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_mollusca(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Apply data augmentation\n",
    "    x = data_augmentation_mollusca(inputs)  #\n",
    "\n",
    "    # Normalize pixel values\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained MobileNetV2 base (frozen)\n",
    "    base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False # Freeze for transfer learning\n",
    "\n",
    "    # Flatten instead of pooling (as required)\n",
    "    x = base_model.output\n",
    "    x = layers.BatchNormalization()(x)               # GlobalAveragePooling2D\n",
    "    x = layers.Dropout(0.3)(x)      # Optional regularization, change, it randomly sets eurons to 0 to reduce overfitting. so 0.1 is 10% of neurons are of.\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.base_model = base_model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d86e8",
   "metadata": {},
   "source": [
    "# Run The models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5533b",
   "metadata": {},
   "source": [
    "## Arthropoda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02699e28",
   "metadata": {},
   "source": [
    "## Chordata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8037e48",
   "metadata": {},
   "source": [
    "### First run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462973a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chordata = make_model_chordata(input_shape=image_size + (3,), num_classes=166)\n",
    "epochs = 10\n",
    "\n",
    "callbacks = [\n",
    "    # saves the best model of the run using max val_accuracy as a metric\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model_chordata.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        verbose=1)\n",
    "    ]\n",
    "\n",
    "## change from kera example is the loss function as we deal with a lot of classes\n",
    "model_chordata.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), ## change this CategoricalCrossentropy to the the one it is now\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")], ## change this CategoricalCrossentropy to the the one it is now\n",
    ")\n",
    "\n",
    "model_chordata.fit(\n",
    "    train_ds_chordata,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_chordata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7c408",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 20\n",
    "\n",
    "# we recall the model only this time we allow it to change the layers in the base model\n",
    "# we load the weights of the best reuslt of the first training\n",
    "fine_tune_model = make_model_chordata(input_shape=image_size + (3,), num_classes=166)\n",
    "fine_tune_model.load_weights(\"best_model_chordata.keras\")\n",
    "\n",
    "# only unfreeze the lasts layer of the pretrained model here 20\n",
    "fine_tune_model.base_model.trainable = True\n",
    "for layer in fine_tune_model.base_model.layers[:-40]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "fine_tune_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4), # lower learning rate\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "fine_tune_model.fit(\n",
    "    train_ds_chordata,\n",
    "    epochs=fine_tune_epochs,\n",
    "    validation_data=val_chordata,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745c90c",
   "metadata": {},
   "source": [
    "## Cnidaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848cd83",
   "metadata": {},
   "source": [
    "## Mollusca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e982eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5d83ef6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b664e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(model_path, test_ds):\n",
    "\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred_probs = model.predict(test_ds)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
