{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b494e3",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In this notebook we make the final ruun of the preoject the idea is to aggregate all the model build for each phylum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd39aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
    "extract_path = '/content/rare_species 1'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Rescaling, RandAugment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# With colab\n",
    "folder_path = '/content/rare_species 1/rare_species 1'\n",
    "meta = pd.read_csv('/content/rare_species 1/rare_species 1/metadata.csv')\n",
    "\n",
    "\n",
    "# With vscode\n",
    "# folder_path = '../data/rare_species 1'\n",
    "# meta = pd.read_csv('../data/rare_species 1/metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854840f",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460b785",
   "metadata": {},
   "source": [
    "## Creating a test set\n",
    "\n",
    "In order for the to by able to have a test set that is not split by phylum we need to do it before spliting each images into a folder for their specific Phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_val , meta_test = train_test_split(meta, test_size=0.1, stratify=meta['category'], random_state=42, stratify =meta['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1734b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "current_locations = '/content/rare_species 1/rare_species 1'\n",
    "\n",
    "# with vscode\n",
    "# current_locations = '../data/rare_species 1'\n",
    "\n",
    "for _, row in meta_train_val.iterrows():\n",
    "\n",
    "    phylum = row['phylum']\n",
    "    file_path = row['file_path']\n",
    "\n",
    "\n",
    "    file_location = os.path.join(current_locations, file_path)\n",
    "\n",
    "    # create a a detination folder keeping the subfolder structure\n",
    "\n",
    "        # with colab\n",
    "    target_folder = os.path.join(phylum, os.path.dirname(file_path))\n",
    "\n",
    "        # with vscode\n",
    "    # target_folder = os.path.join(\"../data\" , phylum, os.path.dirname(file_path))\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "    # Final destination path\n",
    "    destination = os.path.join(target_folder, os.path.basename(file_path))\n",
    "\n",
    "    # Copy the file if it exists\n",
    "    if os.path.exists(file_location):\n",
    "        shutil.copy2(file_location, destination)\n",
    "    else:\n",
    "        print(f\"Couldn't find the file: {file_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3be99",
   "metadata": {},
   "source": [
    "# Final Train, Val, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with colab\n",
    "path_phylum_athropoda = \"/content/arthropoda\"\n",
    "path_phylum_chordata = \"/content/chordata\"\n",
    "path_phylum_cnidaria = \"/content/cnidaria\"\n",
    "path_phylum_mollusca = \"/content/mollusca\"\n",
    "\n",
    "# with vscode\n",
    "# path_phylum_athropoda = \"../data/arthropoda\"\n",
    "# path_phylum_chordata = \"../data/chordata\"\n",
    "# path_phylum_cnidaria = \"../data/cnidaria\"\n",
    "# path_phylum_mollusca = \"../data/mollusca\"\n",
    "\n",
    "image_size = (224, 224)\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "train_ds_athropoda, val_athropoda= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_athropoda,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_chordata, val_chordata= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_chordata,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_cnidaria, val_cnidaria= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_cnidaria,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_mollusca, val_mollusca= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_mollusca,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(train_ds):\n",
    "    for images, labels in train_ds.take(1):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "            plt.title(int(labels[i]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(train_ds_athropoda)\n",
    "display_images(train_ds_chordata)\n",
    "display_images(train_ds_cnidaria)\n",
    "display_images(train_ds_mollusca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818d203",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(images, augmentation_layers):\n",
    "    for layer in augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc238da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_athropoda= [\n",
    " ## add any you want here\n",
    "]\n",
    "\n",
    "data_augmentation_chordata= [\n",
    " ## add any you want here\n",
    "]\n",
    "data_augmentation_cnidaria= [\n",
    "\n",
    " ## add any you want here\n",
    "]\n",
    "data_augmentation_mollusca= [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e88b69",
   "metadata": {},
   "source": [
    "# Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_athropoda(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_chordata(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_cnidaria(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_mollusca(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a480815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_ds, val_ds, epochs=20):\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f\"best_model_{model_name}.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "    run = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_ds,\n",
    "    )\n",
    "    \n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d86e8",
   "metadata": {},
   "source": [
    "# Run The models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d83ef6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b664e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(model_path, test_ds):\n",
    "\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred_probs = model.predict(test_ds)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
