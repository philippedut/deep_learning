{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b494e3",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In this notebook we make the final ruun of the preoject the idea is to aggregate all the model build for each phylum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd39aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
    "extract_path = '/content/rare_species 1'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Rescaling, RandAugment\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "folder_path = '/content/rare_species 1/rare_species 1'\n",
    "meta = pd.read_csv('/content/rare_species 1/rare_species 1/metadata.csv')\n",
    "\n",
    "# With vscode\n",
    "# folder_path = '../data/rare_species 1'\n",
    "# meta = pd.read_csv('../data/rare_species 1/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1734b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "current_locations = '/content/rare_species 1/rare_species 1'\n",
    "\n",
    "# with vscode\n",
    "# current_locations = '../data/rare_species 1'\n",
    "\n",
    "for _, row in meta.iterrows():\n",
    "\n",
    "    phylum = row['phylum']\n",
    "    file_path = row['file_path']\n",
    "\n",
    "\n",
    "    file_location = os.path.join(current_locations, file_path)\n",
    "\n",
    "    # create a a detination folder keeping the subfolder structure\n",
    "\n",
    "        # with colab\n",
    "    target_folder = os.path.join(phylum, os.path.dirname(file_path))\n",
    "\n",
    "        # with vscode\n",
    "    # target_folder = os.path.join(\"../data\" , phylum, os.path.dirname(file_path))\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "    # Final destination path\n",
    "    destination = os.path.join(target_folder, os.path.basename(file_path))\n",
    "\n",
    "    # Copy the file if it exists\n",
    "    if os.path.exists(file_location):\n",
    "        shutil.copy2(file_location, destination)\n",
    "    else:\n",
    "        print(f\"Couldn't find the file: {file_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3be99",
   "metadata": {},
   "source": [
    "# Final Train, Val, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with colab\n",
    "path_phylum_athropoda = \"/content/arthropoda\"\n",
    "path_phylum_chordata = \"/content/chordata\"\n",
    "path_phylum_cnidaria = \"/content/cnidaria\"\n",
    "path_phylum_mollusca = \"/content/mollusca\"\n",
    "\n",
    "# with vscode\n",
    "# path_phylum_athropoda = \"../data/arthropoda\"\n",
    "# path_phylum_chordata = \"../data/chordata\"\n",
    "# path_phylum_cnidaria = \"../data/cnidaria\"\n",
    "# path_phylum_mollusca = \"../data/mollusca\"\n",
    "\n",
    "image_size = (224, 224)\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "train_ds_athropoda, val_test_athropoda= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_athropoda,\n",
    "    validation_split=0.3,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "# the validation set will be about 20% and the test set will be about 10% of the toatal dataset\n",
    "val_batches_athropoda = int(len(val_test_athropoda) * 0.66)\n",
    "val_ds_athropoda = val_test_athropoda.take(val_batches_athropoda)\n",
    "test_ds_athropoda = val_test_athropoda.skip(val_batches_athropoda)\n",
    "\n",
    "train_ds_chordata, val_test_chordata= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_chordata,\n",
    "    validation_split=0.3,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "# the validation set will be about 20% and the test set will be about 10% of the toatal dataset\n",
    "val_batches_chordata = int(len(val_test_chordata) * 0.66)\n",
    "val_ds_chordata = val_test_chordata.take(val_batches_chordata)\n",
    "test_ds_chordata = val_test_chordata.skip(val_batches_chordata)\n",
    "\n",
    "train_ds_cnidaria, val_test_cnidaria= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_cnidaria,\n",
    "    validation_split=0.3,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "# the validation set will be about 20% and the test set will be about 10% of the toatal dataset\n",
    "val_batches_cnidaria = int(len(val_test_cnidaria) * 0.66)\n",
    "val_ds_cnidaria = val_test_cnidaria.take(val_batches_cnidaria)\n",
    "test_ds_cnidaria = val_test_cnidaria.skip(val_batches_cnidaria)\n",
    "\n",
    "train_ds_mollusca, val_test_mollusca= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_mollusca,\n",
    "    validation_split=0.3,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "# the validation set will be about 20% and the test set will be about 10% of the toatal dataset\n",
    "val_batches_mollusca = int(len(val_test_mollusca) * 0.66)\n",
    "val_ds_mollusca = val_test_mollusca.take(val_batches_mollusca)\n",
    "test_ds_mollusca = val_test_mollusca.skip(val_batches_mollusca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(train_ds):\n",
    "    for images, labels in train_ds.take(1):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "            plt.title(int(labels[i]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(train_ds_athropoda)\n",
    "display_images(train_ds_chordata)\n",
    "display_images(train_ds_cnidaria)\n",
    "display_images(train_ds_mollusca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818d203",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(images, augmentation_layers):\n",
    "    for layer in augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc238da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_athropoda= [\n",
    " ## add any you want here\n",
    "]\n",
    "\n",
    "data_augmentation_chordata= [\n",
    " ## add any you want here\n",
    "]\n",
    "data_augmentation_cnidaria= [\n",
    "\n",
    " ## add any you want here\n",
    "]\n",
    "data_augmentation_mollusca= [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e88b69",
   "metadata": {},
   "source": [
    "# Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_athropoda(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_chordata(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_cnidaria(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_mollusca(input_shape, num_classes):\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a480815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_ds, val_ds, epochs=20):\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    # Callbacks: Save best model based on validation accuracy\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f\"best_model_{model_name}.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    run = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_ds,\n",
    "    )\n",
    "    \n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d86e8",
   "metadata": {},
   "source": [
    "# Run The models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d83ef6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b664e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(model_path, test_ds):\n",
    "\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred_probs = model.predict(test_ds)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
