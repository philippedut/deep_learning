{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2289a39b",
   "metadata": {},
   "source": [
    "In the file we are trying to see which pretrained model gives us the best result and therefore which one we are trying to optomize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4966d5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
    "extract_path = '/content/rare_species 1'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.layers import RandAugment\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "folder_path = '/content/rare_species 1/rare_species 1'\n",
    "meta = pd.read_csv('/content/rare_species 1/rare_species 1/metadata.csv')\n",
    "\n",
    "# With vscode\n",
    "# folder_path = '../data/rare_species 1'\n",
    "# meta = pd.read_csv('../data/rare_species 1/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca51679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the diferent Phylum are: \\n{meta['phylum'].unique()}\")\n",
    "print(f\"each phylum contains :  \\n{meta['phylum'].value_counts()}\")\n",
    "\n",
    "print(f\"their is {meta['family'].nunique()} different families\")\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a498b",
   "metadata": {},
   "source": [
    "This codes splits the species to put them on sperate folders for the different phylum in that sense we can build separate model for the different phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06107ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "current_locations = '/content/rare_species 1/rare_species 1'\n",
    "\n",
    "# with vscode\n",
    "# current_locations = '../data/rare_species 1'\n",
    "\n",
    "for _, row in meta.iterrows():\n",
    "\n",
    "    phylum = row['phylum']\n",
    "    file_path = row['file_path']\n",
    "\n",
    "\n",
    "    file_location = os.path.join(current_locations, file_path)\n",
    "\n",
    "    # create a a detination folder keeping the subfolder structure\n",
    "\n",
    "        # with colab\n",
    "    target_folder = os.path.join(phylum, os.path.dirname(file_path))\n",
    "\n",
    "        # with vscode\n",
    "    # target_folder = os.path.join(\"../data\" , phylum, os.path.dirname(file_path))\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "    # Final destination path\n",
    "    destination = os.path.join(target_folder, os.path.basename(file_path))\n",
    "\n",
    "    # Copy the file if it exists\n",
    "    if os.path.exists(file_location):\n",
    "        shutil.copy2(file_location, destination)\n",
    "    else:\n",
    "        print(f\"Couldn't find the file: {file_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e03bdd",
   "metadata": {},
   "source": [
    "# Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a05fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with colab\n",
    "path_phylum_athropoda = \"/content/arthropoda\"\n",
    "path_phylum_chordata = \"/content/chordata\"\n",
    "path_phylum_cnidaria = \"/content/cnidaria\"\n",
    "path_phylum_mollusca = \"/content/mollusca\"\n",
    "\n",
    "# with vscode\n",
    "# path_phylum_athropoda = \"../data/arthropoda\"\n",
    "# path_phylum_chordata = \"../data/chordata\"\n",
    "# path_phylum_cnidaria = \"../data/cnidaria\"\n",
    "# path_phylum_mollusca = \"../data/mollusca\"\n",
    "\n",
    "image_size = (224, 224)\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "train_ds_athoropa, val_ds_athropoda= keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_athropoda,\n",
    "    validation_split=0.2,\n",
    "    subset= \"both\",\n",
    "    seed= seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_chordata, val_ds_chordata = keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_chordata,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_cnidaria, val_ds_cnidaria = keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_cnidaria,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "train_ds_mollusca, val_ds_mollusca = keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_mollusca,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7cef1",
   "metadata": {},
   "source": [
    "# Defining the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d856dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, Xception, DenseNet121\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation functions for different architectures\n",
    "def make_model_vgg16(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = RandAugment(value_range= (0, 255))(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained VGG16 base\n",
    "    base_model = VGG16(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def make_model_resnet50(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = RandAugment(value_range= (0, 255))(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained ResNet50 base\n",
    "    base_model = ResNet50(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def make_model_mobilenetv2(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = RandAugment(value_range= (0, 255))(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained MobileNetV2 base\n",
    "    base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def make_model_xception(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = RandAugment(value_range= (0, 255))(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained Xception base\n",
    "    base_model = Xception(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def make_model_densenet121(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = RandAugment(value_range= (0, 255))(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained DenseNet121 base\n",
    "    base_model = DenseNet121(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cc0e3",
   "metadata": {},
   "source": [
    "# Train and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, train_ds, val_ds, epochs=50):\n",
    "    \"\"\"Train and evaluate a model, saving the best version\"\"\"\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_path = f\"best_model_{model_name}.keras\"\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_ds,\n",
    "    )\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_probs = best_model.predict(val_ds)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Get true labels\n",
    "    y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Return metrics and paths\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'history': history.history,\n",
    "        'accuracy': report['accuracy'],\n",
    "        'f1_macro': report['macro avg']['f1-score'],\n",
    "        'f1_weighted': report['weighted avg']['f1-score'],\n",
    "        'model_path': checkpoint_path\n",
    "    }\n",
    "\n",
    "def compare_models(results, dataset_name):\n",
    "    \"\"\"Compare results from multiple models\"\"\"\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    comparison = pd.DataFrame([\n",
    "        {'Model': r['model_name'],\n",
    "         'Accuracy': r['accuracy'],\n",
    "         'F1 (Macro)': r['f1_macro'],\n",
    "         'F1 (Weighted)': r['f1_weighted']}\n",
    "        for r in results\n",
    "    ])\n",
    "\n",
    "    # Sort by accuracy\n",
    "    comparison = comparison.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n=== Model Comparison for {dataset_name} ===\")\n",
    "    print(comparison)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot accuracy comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(comparison['Model'], comparison['Accuracy'])\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Plot F1 comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(comparison['Model'], comparison['F1 (Weighted)'])\n",
    "    plt.title('F1 Score (Weighted) Comparison')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'model_comparison_{dataset_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Print best model\n",
    "    best_model = comparison.iloc[0]\n",
    "    print(f\"\\nBest model for {dataset_name}: {best_model['Model']}\")\n",
    "    print(f\"Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {best_model['F1 (Weighted)']:.4f}\")\n",
    "\n",
    "    return comparison\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curves(results, dataset_name):\n",
    "    \"\"\"Plot learning curves for all models\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for result in results:\n",
    "        plt.plot(result['history']['acc'], label=f\"{result['model_name']} (Train)\")\n",
    "        plt.plot(result['history']['val_acc'], label=f\"{result['model_name']} (Val)\", linestyle='--')\n",
    "\n",
    "    plt.title(f'Accuracy - {dataset_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for result in results:\n",
    "        plt.plot(result['history']['loss'], label=f\"{result['model_name']} (Train)\")\n",
    "        plt.plot(result['history']['val_loss'], label=f\"{result['model_name']} (Val)\", linestyle='--')\n",
    "\n",
    "    plt.title(f'Loss - {dataset_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'learning_curves_{dataset_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb099aed",
   "metadata": {},
   "source": [
    "# Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20 \n",
    "\n",
    "\n",
    "model_vgg16_chordata = make_model_vgg16(input_shape=image_size + (3,), num_classes=166)\n",
    "model_resnet50_chordata = make_model_resnet50(input_shape=image_size + (3,), num_classes=166)\n",
    "model_mobilenet_chordata = make_model_mobilenetv2(input_shape=image_size + (3,), num_classes=166)\n",
    "\n",
    "# Train and evaluate models\n",
    "results_chordata = []\n",
    "\n",
    "# Train VGG16\n",
    "print(\"\\n=== Training VGG16 on chordata dataset ===\")\n",
    "result_vgg16 = train_and_evaluate_model(\n",
    "    model=model_vgg16_chordata,\n",
    "    model_name=\"vgg16\",\n",
    "    train_ds=train_ds_chordata,\n",
    "    val_ds=val_ds_chordata,\n",
    "    epochs=epochs\n",
    ")\n",
    "results_chordata.append(result_vgg16)\n",
    "\n",
    "# Train ResNet50\n",
    "print(\"\\n=== Training ResNet50 on chordata dataset ===\")\n",
    "result_resnet50 = train_and_evaluate_model(\n",
    "    model=model_resnet50_chordata,\n",
    "    model_name=\"resnet50\",\n",
    "    train_ds=train_ds_chordata,\n",
    "    val_ds=val_ds_chordata,\n",
    "    epochs=epochs\n",
    ")\n",
    "results_chordata.append(result_resnet50)\n",
    "\n",
    "# Train MobileNetV2\n",
    "print(\"\\n=== Training MobileNetV2 on chordata dataset ===\")\n",
    "result_mobilenet = train_and_evaluate_model(\n",
    "    model=model_mobilenet_chordata,\n",
    "    model_name=\"mobilenetv2\",\n",
    "    train_ds=train_ds_chordata,\n",
    "    val_ds=val_ds_chordata,\n",
    "    epochs=epochs\n",
    ")\n",
    "results_chordata.append(result_mobilenet)\n",
    "\n",
    "# Compare models\n",
    "compare_models(results_chordata, \"Chordata\")\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(results_chordata, \"Chordata\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
