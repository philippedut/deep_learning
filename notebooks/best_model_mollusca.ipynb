{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fb6aac",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc9b35",
   "metadata": {},
   "source": [
    "in this notebook we are trying to optimise the best model form the selection phase to better target each different phylum. In that sense we are splitting the training and the modeling for the different phylumns  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb015a6a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156908b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
    "extract_path = '/content/rare_species 1'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Rescaling, RandAugment\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "folder_path = '/content/rare_species 1/rare_species 1'\n",
    "meta = pd.read_csv('/content/rare_species 1/rare_species 1/metadata.csv')\n",
    "\n",
    "# With vscode\n",
    "# folder_path = '../data/rare_species 1'\n",
    "# meta = pd.read_csv('../data/rare_species 1/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the diferent Phylum are: \\n{meta['phylum'].unique()}\")\n",
    "print(f\"each phylum contains :  \\n{meta['phylum'].value_counts()}\")\n",
    "\n",
    "print(f\"their is {meta['family'].nunique()} different families\")\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f482a0",
   "metadata": {},
   "source": [
    "# Phylum Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f288916",
   "metadata": {},
   "source": [
    "This code splits the species into separate folders based on their phylum.  \n",
    "This organization allows us to train a dedicated model for each phylum more effectively.<br>\n",
    "(we repeat this proses in each notebook only because we are using colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5139bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With colab\n",
    "current_locations = '/content/rare_species 1/rare_species 1'\n",
    "\n",
    "# with vscode\n",
    "# current_locations = '../data/rare_species 1'\n",
    "\n",
    "for _, row in meta.iterrows():\n",
    "\n",
    "    phylum = row['phylum']\n",
    "    file_path = row['file_path']\n",
    "\n",
    "\n",
    "    file_location = os.path.join(current_locations, file_path)\n",
    "\n",
    "    # create a a detination folder keeping the subfolder structure\n",
    "\n",
    "        # with colab\n",
    "    target_folder = os.path.join(phylum, os.path.dirname(file_path))\n",
    "\n",
    "        # with vscode\n",
    "    # target_folder = os.path.join(\"../data\" , phylum, os.path.dirname(file_path))\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "    # Final destination path\n",
    "    destination = os.path.join(target_folder, os.path.basename(file_path))\n",
    "\n",
    "    # Copy the file if it exists\n",
    "    if os.path.exists(file_location):\n",
    "        shutil.copy2(file_location, destination)\n",
    "    else:\n",
    "        print(f\"Couldn't find the file: {file_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5d283",
   "metadata": {},
   "source": [
    "# Train Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d127908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with colab\n",
    "path_phylum_chordata = \"/content/chordata\"\n",
    "\n",
    "\n",
    "# with vscode\n",
    "# path_phylum_chordata = \"../data/chordata\"\n",
    "\n",
    "\n",
    "image_size = (224, 224)\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_ds_chordata, val_ds_chordata = keras.utils.image_dataset_from_directory(\n",
    "    path_phylum_chordata,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=seed,\n",
    "    image_size= image_size,\n",
    "    batch_size= batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(train_ds):\n",
    "    for images, labels in train_ds.take(1):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "            plt.title(int(labels[i]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "display_images(train_ds_chordata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236b02f",
   "metadata": {},
   "source": [
    "# Augmentation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef60d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layers = [\n",
    "    RandAugment(value_range= (0, 255))\n",
    " ## add any you want here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfca3fb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to be adapted to the new dataset\n",
    "def make_model_mobilenetv2(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Rescaling layer\n",
    "    x = data_augmentation(inputs)(inputs)\n",
    "    x = Rescaling(1./255)(x)\n",
    "\n",
    "    # Pretrained MobileNetV2 base\n",
    "    base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    base_model.trainable = False  # Freeze for transfer learning\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Optional regularization\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_athoropa = make_model_mobilenetv2(input_shape=image_size + (3,), num_classes=166)\n",
    "keras.utils.plot_model(model_athoropa, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7444616",
   "metadata": {},
   "source": [
    "# Run and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197394bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "# make the learning rate change over the epochs to avoid getting stuck\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    # saves the best model of the run using max val_accuracy as a metric\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model_chordata.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        verbose=1)\n",
    "    ]\n",
    "\n",
    "## change from kera example is the loss function as we deal with a lot of classes\n",
    "model_athoropa.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), ## change this CategoricalCrossentropy to the the one it is now\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")], ## change this CategoricalCrossentropy to the the one it is now\n",
    ")\n",
    "\n",
    "model_athoropa.fit(\n",
    "    train_ds_chordata,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds_chordata,\n",
    ")\n",
    "\n",
    "# get the clasification report\n",
    "best_model_example = keras.models.load_model(\"best_model_chordata.keras\")\n",
    "y_pred_probs = best_model_example.predict(val_ds_chordata)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in val_ds_chordata], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
