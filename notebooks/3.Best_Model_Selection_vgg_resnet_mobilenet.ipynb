{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2289a39b",
      "metadata": {
        "id": "2289a39b"
      },
      "source": [
        "# Pretrained Model Comparison\n",
        "\n",
        "In this notebook, we aim to identify the best-performing pretrained model.\n",
        "\n",
        "- The same preprocessing steps and model pipeline were used for each model to ensure a fair comparison.\n",
        "- To build the pretrained models, we used a pipeline similar to the one from the practical class, adapted to fit the function used for `model_from_scratch`.  \n",
        "  This includes:  \n",
        "  - An augmentation layer (again simple one)\n",
        "  - A rescaling layer  \n",
        "  - A pretrained model without top layers  \n",
        "  - A Flatten layer  \n",
        "  - A Dropout layer (added by us, as we observed it could reduce overfitting)  \n",
        "  - A Dense output layer\n",
        "\n",
        "We used the following models:\n",
        "- **VGG16**, as it was presented in the practical class  \n",
        "- **ResNet50**, as we saw it is a robust model for complex image classification tasks  \n",
        "- **MobileNetV2**, as it is known to be a very efficient and fast model\n",
        "\n",
        "We also decided to **freeze the pretrained layers**, as it is considered good practice when using transfer learning.  \n",
        "In future iterations, we plan to experiment with unfreezing the layers after the initial training phase.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "From this run, we can conclude that the pretrained model **MobileNetV2** performs better in every aspect compared to the others.  \n",
        "Therefore, it is likely that we will choose this model for the next steps.\n",
        "\n",
        "Although we are aware that this could be due to our pipeline being more tailored to this specific model, the results are promising.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4966d5",
      "metadata": {
        "id": "da4966d5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328d7d93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "328d7d93",
        "outputId": "82e05d79-43c9-4a43-9456-c672cf453377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/rare_species 1.zip'\n",
        "extract_path = '/content/rare_species 1'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440a7b63",
      "metadata": {
        "id": "440a7b63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, Xception, DenseNet121\n",
        "from tensorflow.keras.layers import Rescaling, RandAugment\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b717191a",
      "metadata": {
        "id": "b717191a"
      },
      "outputs": [],
      "source": [
        "# With colab\n",
        "folder_path = '/content/rare_species 1'\n",
        "meta = pd.read_csv('/content/rare_species 1/metadata.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e03bdd",
      "metadata": {
        "id": "97e03bdd"
      },
      "source": [
        "# Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a05fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a05fcd",
        "outputId": "25e6436e-a36a-4d07-df4a-e90eb4ad3553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 11983 files belonging to 202 classes.\n",
            "Using 9587 files for training.\n",
            "Using 2396 files for validation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_size = (224, 224)\n",
        "seed = 42\n",
        "batch_size = 32\n",
        "\n",
        "train_ds, val_ds= keras.utils.image_dataset_from_directory(\n",
        "    folder_path,\n",
        "    validation_split=0.2,\n",
        "    subset= \"both\",\n",
        "    seed= seed,\n",
        "    image_size= image_size,\n",
        "    batch_size= batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded7cef1",
      "metadata": {
        "id": "ded7cef1"
      },
      "source": [
        "# Defining the different models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc233082",
      "metadata": {
        "id": "fc233082"
      },
      "source": [
        "In this section, we create three different models. All models are built using the same architecture to allow for fair comparison. The pipeline for each model includes the following components:\n",
        "\n",
        "- **Augmentation layer**: Applies basic random transformations to simulate data variability.\n",
        "- **Rescaling layer**: Normalizes pixel values.\n",
        "- **Pretrained model**: Varies between the models to test performance differences.\n",
        "- **Flatten layer**: Converts the output of the convolutional base to a 1D vector.\n",
        "- **Dropout layer**: Helps the model generalize better by reducing overfitting.\n",
        "\n",
        "Although this setup is not optimal—since different pretrained models may respond better to different image sizes and configurations—we believe this approach provides a consistent and efficient basis for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d856dd7",
      "metadata": {
        "id": "9d856dd7"
      },
      "outputs": [],
      "source": [
        "# Model creation functions for different architectures\n",
        "def make_model_vgg16(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = RandAugment(value_range= (0, 255))(inputs)\n",
        "    x = Rescaling(1./255)(x)\n",
        "\n",
        "    # Pretrained VGG16\n",
        "    base_model = VGG16(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    base_model.trainable = False  # Freeze for transfer learning\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.1)(x) # To somewhat prevent overfitting though it might be to little\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def make_model_resnet50(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = RandAugment(value_range= (0, 255))(inputs)\n",
        "    x = Rescaling(1./255)(x)\n",
        "\n",
        "    # Pretrained ResNet50\n",
        "    base_model = ResNet50(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    base_model.trainable = False  # Freeze for transfer learning\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.1)(x) # To somewhat prevent overfitting though it might be to little\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def make_model_mobilenetv2(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = RandAugment(value_range= (0, 255))(inputs)\n",
        "    x = Rescaling(1./255)(x)\n",
        "\n",
        "    # Pretrained MobileNetV2\n",
        "    base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "    base_model.trainable = False  # Freeze for transfer learning\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.1)(x) # To somewhat prevent overfitting though it might be to little\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01cc0e3",
      "metadata": {
        "id": "e01cc0e3"
      },
      "source": [
        "# Train and evaluate the models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6530fda",
      "metadata": {
        "id": "d6530fda"
      },
      "source": [
        "For each of the model we save only the model with the best score on val in order to compare the models. if one model as a very high val-score it doesn't mean it will be picked as other parameter like over/under-fitting, loss is taken into account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92b4130",
      "metadata": {
        "id": "a92b4130"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, model_name, train_ds, val_ds, epochs=50):\n",
        "    \"\"\"Train and evaluate a model, saving the best version\"\"\"\n",
        "\n",
        "    checkpoint_path = f\"best_model_{model_name}.keras\"\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            checkpoint_path,\n",
        "            save_best_only=True,\n",
        "            monitor=\"val_acc\",\n",
        "            mode=\"max\",\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3), # larger learning rate then the model form scratch as it was mentioned in class\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=val_ds,\n",
        "    )\n",
        "\n",
        "    # load the best model and make a final prediction on val\n",
        "    best_model = keras.models.load_model(checkpoint_path)\n",
        "    y_pred_probs = best_model.predict(val_ds)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "\n",
        "    # Print classification report\n",
        "    print(f\"\\nClassification Report for {model_name}:\")\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Return metrics and paths\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'history': history.history,\n",
        "        'accuracy': report['accuracy'],\n",
        "        'f1_macro': report['macro avg']['f1-score'],\n",
        "        'f1_weighted': report['weighted avg']['f1-score'],\n",
        "        'model_path': checkpoint_path\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jDLuo0jQ1wCl",
      "metadata": {
        "id": "jDLuo0jQ1wCl"
      },
      "source": [
        "# Model comparasion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb099aed",
      "metadata": {
        "id": "eb099aed"
      },
      "source": [
        "# Model run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba5d572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ba5d572",
        "outputId": "43f8d160-0646-4feb-a1f4-cd533719662e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-bf053a51f449>:48: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(include_top=False, input_tensor=x, weights=\"imagenet\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "VGG16:\n",
            "Epoch 1/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - acc: 0.0964 - loss: 6.5189\n",
            "Epoch 1: val_acc improved from -inf to 0.16694, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 213ms/step - acc: 0.0965 - loss: 6.5173 - val_acc: 0.1669 - val_loss: 5.5781\n",
            "Epoch 2/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.2925 - loss: 4.2564\n",
            "Epoch 2: val_acc improved from 0.16694 to 0.17362, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 204ms/step - acc: 0.2925 - loss: 4.2568 - val_acc: 0.1736 - val_loss: 5.8831\n",
            "Epoch 3/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.3733 - loss: 3.5363\n",
            "Epoch 3: val_acc improved from 0.17362 to 0.20117, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 205ms/step - acc: 0.3733 - loss: 3.5368 - val_acc: 0.2012 - val_loss: 5.9320\n",
            "Epoch 4/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.4249 - loss: 3.2478\n",
            "Epoch 4: val_acc improved from 0.20117 to 0.22454, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - acc: 0.4249 - loss: 3.2479 - val_acc: 0.2245 - val_loss: 5.7467\n",
            "Epoch 5/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.4822 - loss: 2.8285\n",
            "Epoch 5: val_acc improved from 0.22454 to 0.23372, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 205ms/step - acc: 0.4821 - loss: 2.8286 - val_acc: 0.2337 - val_loss: 5.8876\n",
            "Epoch 6/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - acc: 0.5215 - loss: 2.5725\n",
            "Epoch 6: val_acc improved from 0.23372 to 0.27003, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 205ms/step - acc: 0.5215 - loss: 2.5725 - val_acc: 0.2700 - val_loss: 5.6648\n",
            "Epoch 7/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.5514 - loss: 2.2859\n",
            "Epoch 7: val_acc did not improve from 0.27003\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.5514 - loss: 2.2863 - val_acc: 0.2634 - val_loss: 5.8161\n",
            "Epoch 8/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.5677 - loss: 2.2697\n",
            "Epoch 8: val_acc did not improve from 0.27003\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.5677 - loss: 2.2697 - val_acc: 0.2554 - val_loss: 6.1768\n",
            "Epoch 9/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6040 - loss: 1.9873\n",
            "Epoch 9: val_acc did not improve from 0.27003\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.6040 - loss: 1.9874 - val_acc: 0.2458 - val_loss: 6.3093\n",
            "Epoch 10/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6153 - loss: 1.9949\n",
            "Epoch 10: val_acc did not improve from 0.27003\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 202ms/step - acc: 0.6153 - loss: 1.9949 - val_acc: 0.2433 - val_loss: 6.8832\n",
            "Epoch 11/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6354 - loss: 1.8340\n",
            "Epoch 11: val_acc improved from 0.27003 to 0.27796, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - acc: 0.6354 - loss: 1.8340 - val_acc: 0.2780 - val_loss: 6.2193\n",
            "Epoch 12/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - acc: 0.6478 - loss: 1.8083\n",
            "Epoch 12: val_acc did not improve from 0.27796\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.6478 - loss: 1.8085 - val_acc: 0.2771 - val_loss: 6.6650\n",
            "Epoch 13/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6547 - loss: 1.7456\n",
            "Epoch 13: val_acc improved from 0.27796 to 0.29090, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - acc: 0.6548 - loss: 1.7455 - val_acc: 0.2909 - val_loss: 6.4069\n",
            "Epoch 14/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6920 - loss: 1.4865\n",
            "Epoch 14: val_acc did not improve from 0.29090\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.6919 - loss: 1.4866 - val_acc: 0.2667 - val_loss: 6.9683\n",
            "Epoch 15/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - acc: 0.6853 - loss: 1.5515\n",
            "Epoch 15: val_acc did not improve from 0.29090\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - acc: 0.6853 - loss: 1.5515 - val_acc: 0.2730 - val_loss: 6.8668\n",
            "Epoch 16/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.6922 - loss: 1.4676\n",
            "Epoch 16: val_acc did not improve from 0.29090\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.6922 - loss: 1.4676 - val_acc: 0.2788 - val_loss: 7.3947\n",
            "Epoch 17/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.7065 - loss: 1.4675\n",
            "Epoch 17: val_acc did not improve from 0.29090\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.7065 - loss: 1.4674 - val_acc: 0.2813 - val_loss: 7.2310\n",
            "Epoch 18/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.7073 - loss: 1.4649\n",
            "Epoch 18: val_acc improved from 0.29090 to 0.29341, saving model to best_model_vgg16.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 204ms/step - acc: 0.7072 - loss: 1.4650 - val_acc: 0.2934 - val_loss: 7.0108\n",
            "Epoch 19/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - acc: 0.7269 - loss: 1.3581\n",
            "Epoch 19: val_acc did not improve from 0.29341\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 204ms/step - acc: 0.7270 - loss: 1.3580 - val_acc: 0.2863 - val_loss: 7.4015\n",
            "Epoch 20/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - acc: 0.7346 - loss: 1.2621\n",
            "Epoch 20: val_acc did not improve from 0.29341\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 203ms/step - acc: 0.7346 - loss: 1.2624 - val_acc: 0.2876 - val_loss: 7.6706\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 160ms/step\n",
            "\n",
            "Classification Report for vgg16:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.53      0.69        32\n",
            "           1       0.27      0.60      0.38         5\n",
            "           2       0.06      0.25      0.09        12\n",
            "           3       0.29      0.40      0.33         5\n",
            "           4       0.42      1.00      0.59         5\n",
            "           5       0.76      0.82      0.79        51\n",
            "           6       0.57      0.33      0.42        12\n",
            "           7       1.00      0.17      0.29         6\n",
            "           8       1.00      0.43      0.60         7\n",
            "           9       0.00      0.00      0.00         4\n",
            "          10       1.00      0.80      0.89         5\n",
            "          11       1.00      0.20      0.33         5\n",
            "          12       1.00      0.22      0.36         9\n",
            "          13       0.00      0.00      0.00         7\n",
            "          14       0.20      0.67      0.31         3\n",
            "          15       0.67      0.86      0.75         7\n",
            "          16       0.50      0.43      0.46         7\n",
            "          17       0.32      0.34      0.33        35\n",
            "          18       0.00      0.00      0.00        12\n",
            "          19       0.05      0.36      0.09        11\n",
            "          20       0.25      0.17      0.20         6\n",
            "          21       0.00      0.00      0.00         8\n",
            "          22       0.20      0.44      0.28         9\n",
            "          23       0.00      0.00      0.00        11\n",
            "          24       0.06      0.10      0.07        10\n",
            "          25       0.32      0.23      0.27        26\n",
            "          26       1.00      0.20      0.33        15\n",
            "          27       0.38      0.50      0.43        12\n",
            "          28       0.33      0.27      0.30        11\n",
            "          29       0.50      0.29      0.36         7\n",
            "          30       0.21      0.47      0.29        30\n",
            "          31       0.10      0.12      0.11         8\n",
            "          32       0.00      0.00      0.00         3\n",
            "          33       0.25      0.20      0.22        10\n",
            "          34       0.10      0.42      0.16        12\n",
            "          35       0.14      0.14      0.14         7\n",
            "          36       0.28      0.34      0.31        35\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.37      0.19      0.25        37\n",
            "          39       0.30      0.48      0.37        33\n",
            "          40       0.46      0.43      0.44        14\n",
            "          41       0.20      0.17      0.18         6\n",
            "          42       0.44      0.17      0.24        24\n",
            "          43       0.00      0.00      0.00         3\n",
            "          44       0.00      0.00      0.00         7\n",
            "          45       0.33      0.57      0.42        53\n",
            "          46       0.00      0.00      0.00         7\n",
            "          47       0.00      0.00      0.00        18\n",
            "          48       0.36      0.15      0.21        66\n",
            "          49       0.15      0.22      0.18         9\n",
            "          50       0.00      0.00      0.00         4\n",
            "          51       1.00      0.30      0.46        10\n",
            "          52       1.00      0.12      0.22         8\n",
            "          53       0.09      0.57      0.16         7\n",
            "          54       0.50      0.25      0.33         4\n",
            "          55       1.00      0.43      0.60         7\n",
            "          56       0.00      0.00      0.00         3\n",
            "          57       0.00      0.00      0.00         4\n",
            "          58       0.60      0.12      0.21        24\n",
            "          59       0.29      0.52      0.37        21\n",
            "          60       0.00      0.00      0.00         4\n",
            "          61       0.50      0.10      0.17        10\n",
            "          62       0.00      0.00      0.00         4\n",
            "          63       1.00      0.06      0.11        18\n",
            "          64       0.14      0.08      0.10        13\n",
            "          65       0.00      0.00      0.00         6\n",
            "          66       0.00      0.00      0.00         4\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.92      0.35      0.51        62\n",
            "          69       0.00      0.00      0.00         3\n",
            "          70       0.45      0.18      0.26        28\n",
            "          71       1.00      0.50      0.67         8\n",
            "          72       0.00      0.00      0.00         3\n",
            "          73       0.00      0.00      0.00         9\n",
            "          74       0.50      0.19      0.28        26\n",
            "          75       0.50      0.17      0.25         6\n",
            "          76       0.19      0.26      0.22        27\n",
            "          77       0.29      0.12      0.17        17\n",
            "          78       1.00      0.33      0.50         6\n",
            "          79       0.00      0.00      0.00        10\n",
            "          80       1.00      0.33      0.50         6\n",
            "          81       0.75      0.60      0.67         5\n",
            "          82       0.58      0.41      0.48        17\n",
            "          83       0.67      0.50      0.57         4\n",
            "          84       0.06      0.43      0.10         7\n",
            "          85       0.00      0.00      0.00         8\n",
            "          86       0.27      0.42      0.33        19\n",
            "          87       1.00      0.40      0.57        10\n",
            "          88       0.00      0.00      0.00         3\n",
            "          89       0.00      0.00      0.00         7\n",
            "          90       1.00      0.67      0.80         3\n",
            "          91       0.00      0.00      0.00         4\n",
            "          92       0.00      0.00      0.00         8\n",
            "          93       0.00      0.00      0.00        10\n",
            "          94       0.10      0.44      0.16         9\n",
            "          95       0.50      0.22      0.31         9\n",
            "          96       0.38      0.57      0.46        14\n",
            "          97       0.00      0.00      0.00         4\n",
            "          98       0.20      0.46      0.28        26\n",
            "          99       0.14      0.75      0.24         4\n",
            "         100       0.86      0.43      0.57        14\n",
            "         101       0.00      0.00      0.00        12\n",
            "         102       1.00      0.17      0.29         6\n",
            "         103       0.29      0.21      0.24        29\n",
            "         104       0.60      0.33      0.43         9\n",
            "         105       0.50      0.07      0.12        14\n",
            "         106       0.00      0.00      0.00        16\n",
            "         107       0.00      0.00      0.00         9\n",
            "         108       0.00      0.00      0.00        15\n",
            "         109       0.67      0.22      0.33         9\n",
            "         110       1.00      0.17      0.29         6\n",
            "         111       1.00      0.14      0.25         7\n",
            "         112       0.00      0.00      0.00         9\n",
            "         113       0.40      0.50      0.44         4\n",
            "         114       0.00      0.00      0.00         6\n",
            "         115       0.50      0.33      0.40         6\n",
            "         116       0.09      0.24      0.13        21\n",
            "         117       0.42      0.56      0.48        18\n",
            "         118       0.40      0.67      0.50         3\n",
            "         119       0.50      0.43      0.46         7\n",
            "         120       0.04      0.11      0.06         9\n",
            "         121       0.67      0.29      0.40         7\n",
            "         122       0.00      0.00      0.00         9\n",
            "         123       0.13      0.50      0.21         6\n",
            "         124       0.00      0.00      0.00         3\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         6\n",
            "         127       0.10      0.17      0.12         6\n",
            "         128       0.00      0.00      0.00         9\n",
            "         129       0.25      0.40      0.31         5\n",
            "         130       0.00      0.00      0.00         3\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.59      0.36      0.45        64\n",
            "         133       0.33      0.25      0.29         4\n",
            "         134       0.00      0.00      0.00         2\n",
            "         135       0.50      0.29      0.36         7\n",
            "         136       0.12      0.20      0.15         5\n",
            "         137       0.50      0.50      0.50         4\n",
            "         138       0.00      0.00      0.00         4\n",
            "         139       0.50      0.17      0.25        18\n",
            "         140       0.19      0.48      0.27        25\n",
            "         141       1.00      0.25      0.40        12\n",
            "         142       0.62      0.45      0.53        11\n",
            "         143       0.23      0.47      0.30        15\n",
            "         144       0.14      0.25      0.18         4\n",
            "         145       0.00      0.00      0.00         3\n",
            "         146       0.15      0.33      0.21        21\n",
            "         147       1.00      0.25      0.40         8\n",
            "         148       0.06      0.14      0.09         7\n",
            "         149       0.83      0.45      0.59        11\n",
            "         150       0.14      0.28      0.19        18\n",
            "         151       0.41      0.28      0.33        40\n",
            "         152       0.20      0.12      0.15         8\n",
            "         153       0.00      0.00      0.00         6\n",
            "         154       0.78      0.78      0.78         9\n",
            "         155       0.33      0.36      0.35        11\n",
            "         156       1.00      0.33      0.50         3\n",
            "         157       0.17      0.50      0.25        16\n",
            "         158       0.00      0.00      0.00         7\n",
            "         159       0.00      0.00      0.00         6\n",
            "         160       1.00      0.11      0.20         9\n",
            "         161       0.00      0.00      0.00        11\n",
            "         162       0.75      0.12      0.20        26\n",
            "         163       0.13      0.30      0.18        23\n",
            "         164       0.00      0.00      0.00         2\n",
            "         165       0.67      0.50      0.57         4\n",
            "         166       0.50      0.17      0.25         6\n",
            "         167       1.00      0.12      0.22         8\n",
            "         168       0.00      0.00      0.00        14\n",
            "         169       0.50      0.28      0.36        18\n",
            "         170       0.67      0.22      0.33        18\n",
            "         171       0.00      0.00      0.00         3\n",
            "         172       0.26      0.67      0.37        15\n",
            "         173       0.67      0.33      0.44         6\n",
            "         174       0.00      0.00      0.00         8\n",
            "         175       0.00      0.00      0.00         8\n",
            "         176       0.00      0.00      0.00         6\n",
            "         177       0.27      0.30      0.29        10\n",
            "         178       0.00      0.00      0.00         7\n",
            "         179       0.00      0.00      0.00        13\n",
            "         180       1.00      0.40      0.57         5\n",
            "         181       1.00      0.14      0.25         7\n",
            "         182       0.43      0.60      0.50         5\n",
            "         183       0.61      0.46      0.52        48\n",
            "         184       0.22      0.73      0.34        30\n",
            "         185       0.00      0.00      0.00        10\n",
            "         186       0.20      0.33      0.25         9\n",
            "         187       0.71      0.21      0.32        24\n",
            "         188       0.24      0.50      0.32        10\n",
            "         189       0.50      0.50      0.50         6\n",
            "         190       0.17      0.17      0.17         6\n",
            "         191       0.67      0.67      0.67         3\n",
            "         192       0.00      0.00      0.00         8\n",
            "         193       0.14      0.09      0.11        11\n",
            "         194       0.88      0.44      0.58        16\n",
            "         195       1.00      0.11      0.20         9\n",
            "         196       0.36      0.80      0.50        10\n",
            "         197       0.24      0.56      0.33         9\n",
            "         198       1.00      1.00      1.00         5\n",
            "         199       0.25      1.00      0.40         3\n",
            "         200       0.75      0.43      0.55        14\n",
            "         201       0.50      0.45      0.48        11\n",
            "\n",
            "    accuracy                           0.29      2396\n",
            "   macro avg       0.35      0.26      0.25      2396\n",
            "weighted avg       0.40      0.29      0.29      2396\n",
            "\n",
            "Resnet50:\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - acc: 0.0286 - loss: 22.3596\n",
            "Epoch 1: val_acc improved from -inf to 0.06052, saving model to best_model_resnet50.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 192ms/step - acc: 0.0287 - loss: 22.3446 - val_acc: 0.0605 - val_loss: 12.3880\n",
            "Epoch 2/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.0955 - loss: 11.4062\n",
            "Epoch 2: val_acc did not improve from 0.06052\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.0955 - loss: 11.4074 - val_acc: 0.0551 - val_loss: 15.6258\n",
            "Epoch 3/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.1509 - loss: 11.0204\n",
            "Epoch 3: val_acc did not improve from 0.06052\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.1509 - loss: 11.0202 - val_acc: 0.0392 - val_loss: 11.7611\n",
            "Epoch 4/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.1826 - loss: 9.8911\n",
            "Epoch 4: val_acc improved from 0.06052 to 0.09432, saving model to best_model_resnet50.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 176ms/step - acc: 0.1826 - loss: 9.8924 - val_acc: 0.0943 - val_loss: 14.0917\n",
            "Epoch 5/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.1995 - loss: 10.6863\n",
            "Epoch 5: val_acc did not improve from 0.09432\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.1995 - loss: 10.6842 - val_acc: 0.0776 - val_loss: 13.3137\n",
            "Epoch 6/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.2450 - loss: 9.2469\n",
            "Epoch 6: val_acc did not improve from 0.09432\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 171ms/step - acc: 0.2450 - loss: 9.2465 - val_acc: 0.0818 - val_loss: 12.2437\n",
            "Epoch 7/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.2598 - loss: 8.3829\n",
            "Epoch 7: val_acc did not improve from 0.09432\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.2598 - loss: 8.3838 - val_acc: 0.0693 - val_loss: 13.0897\n",
            "Epoch 8/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.2946 - loss: 7.9464\n",
            "Epoch 8: val_acc did not improve from 0.09432\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.2946 - loss: 7.9483 - val_acc: 0.0860 - val_loss: 14.7878\n",
            "Epoch 9/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.2890 - loss: 8.9287\n",
            "Epoch 9: val_acc did not improve from 0.09432\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.2890 - loss: 8.9286 - val_acc: 0.0927 - val_loss: 12.5527\n",
            "Epoch 10/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.3211 - loss: 8.1084\n",
            "Epoch 10: val_acc improved from 0.09432 to 0.11603, saving model to best_model_resnet50.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 176ms/step - acc: 0.3211 - loss: 8.1089 - val_acc: 0.1160 - val_loss: 12.7407\n",
            "Epoch 11/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.3327 - loss: 7.5382\n",
            "Epoch 11: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 171ms/step - acc: 0.3327 - loss: 7.5384 - val_acc: 0.1010 - val_loss: 14.2799\n",
            "Epoch 12/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.3492 - loss: 7.6836\n",
            "Epoch 12: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.3492 - loss: 7.6835 - val_acc: 0.0789 - val_loss: 14.4246\n",
            "Epoch 13/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.3603 - loss: 7.2732\n",
            "Epoch 13: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.3603 - loss: 7.2735 - val_acc: 0.1010 - val_loss: 13.7516\n",
            "Epoch 14/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.3801 - loss: 7.1503\n",
            "Epoch 14: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.3800 - loss: 7.1511 - val_acc: 0.0952 - val_loss: 15.0541\n",
            "Epoch 15/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.3871 - loss: 7.7219\n",
            "Epoch 15: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.3871 - loss: 7.7216 - val_acc: 0.1064 - val_loss: 15.8272\n",
            "Epoch 16/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.3807 - loss: 7.6095\n",
            "Epoch 16: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 171ms/step - acc: 0.3808 - loss: 7.6089 - val_acc: 0.1039 - val_loss: 13.6894\n",
            "Epoch 17/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.4137 - loss: 6.9361\n",
            "Epoch 17: val_acc did not improve from 0.11603\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 171ms/step - acc: 0.4137 - loss: 6.9365 - val_acc: 0.0997 - val_loss: 13.6558\n",
            "Epoch 18/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.4282 - loss: 6.6597\n",
            "Epoch 18: val_acc improved from 0.11603 to 0.12646, saving model to best_model_resnet50.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 177ms/step - acc: 0.4282 - loss: 6.6605 - val_acc: 0.1265 - val_loss: 14.4542\n",
            "Epoch 19/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.4326 - loss: 6.9553\n",
            "Epoch 19: val_acc did not improve from 0.12646\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - acc: 0.4326 - loss: 6.9559 - val_acc: 0.1002 - val_loss: 15.8291\n",
            "Epoch 20/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - acc: 0.4232 - loss: 7.0341\n",
            "Epoch 20: val_acc did not improve from 0.12646\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 171ms/step - acc: 0.4232 - loss: 7.0335 - val_acc: 0.1010 - val_loss: 14.2698\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step\n",
            "\n",
            "Classification Report for resnet50:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.62      0.77        32\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00        12\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.50      0.20      0.29         5\n",
            "           5       0.43      0.65      0.52        51\n",
            "           6       0.00      0.00      0.00        12\n",
            "           7       0.00      0.00      0.00         6\n",
            "           8       0.12      0.14      0.13         7\n",
            "           9       0.00      0.00      0.00         4\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.00      0.00      0.00         7\n",
            "          14       0.00      0.00      0.00         3\n",
            "          15       0.50      0.14      0.22         7\n",
            "          16       0.00      0.00      0.00         7\n",
            "          17       0.06      0.34      0.10        35\n",
            "          18       0.04      0.25      0.07        12\n",
            "          19       0.03      0.09      0.04        11\n",
            "          20       0.00      0.00      0.00         6\n",
            "          21       0.00      0.00      0.00         8\n",
            "          22       0.00      0.00      0.00         9\n",
            "          23       0.00      0.00      0.00        11\n",
            "          24       0.00      0.00      0.00        10\n",
            "          25       0.00      0.00      0.00        26\n",
            "          26       0.00      0.00      0.00        15\n",
            "          27       1.00      0.50      0.67        12\n",
            "          28       0.00      0.00      0.00        11\n",
            "          29       0.00      0.00      0.00         7\n",
            "          30       0.53      0.27      0.36        30\n",
            "          31       0.00      0.00      0.00         8\n",
            "          32       0.00      0.00      0.00         3\n",
            "          33       0.00      0.00      0.00        10\n",
            "          34       0.00      0.00      0.00        12\n",
            "          35       0.00      0.00      0.00         7\n",
            "          36       0.00      0.00      0.00        35\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.75      0.08      0.15        37\n",
            "          39       0.26      0.18      0.21        33\n",
            "          40       0.00      0.00      0.00        14\n",
            "          41       0.00      0.00      0.00         6\n",
            "          42       0.67      0.17      0.27        24\n",
            "          43       0.00      0.00      0.00         3\n",
            "          44       0.00      0.00      0.00         7\n",
            "          45       0.50      0.02      0.04        53\n",
            "          46       0.00      0.00      0.00         7\n",
            "          47       0.02      0.06      0.03        18\n",
            "          48       0.14      0.27      0.19        66\n",
            "          49       0.00      0.00      0.00         9\n",
            "          50       0.00      0.00      0.00         4\n",
            "          51       0.00      0.00      0.00        10\n",
            "          52       0.00      0.00      0.00         8\n",
            "          53       0.00      0.00      0.00         7\n",
            "          54       0.00      0.00      0.00         4\n",
            "          55       0.00      0.00      0.00         7\n",
            "          56       0.00      0.00      0.00         3\n",
            "          57       0.00      0.00      0.00         4\n",
            "          58       0.00      0.00      0.00        24\n",
            "          59       0.00      0.00      0.00        21\n",
            "          60       0.25      0.25      0.25         4\n",
            "          61       0.00      0.00      0.00        10\n",
            "          62       0.00      0.00      0.00         4\n",
            "          63       0.00      0.00      0.00        18\n",
            "          64       0.00      0.00      0.00        13\n",
            "          65       0.00      0.00      0.00         6\n",
            "          66       0.00      0.00      0.00         4\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.24      0.71      0.36        62\n",
            "          69       0.00      0.00      0.00         3\n",
            "          70       0.00      0.00      0.00        28\n",
            "          71       0.00      0.00      0.00         8\n",
            "          72       0.00      0.00      0.00         3\n",
            "          73       0.00      0.00      0.00         9\n",
            "          74       0.00      0.00      0.00        26\n",
            "          75       0.00      0.00      0.00         6\n",
            "          76       0.03      0.48      0.05        27\n",
            "          77       0.00      0.00      0.00        17\n",
            "          78       0.33      0.33      0.33         6\n",
            "          79       0.00      0.00      0.00        10\n",
            "          80       0.00      0.00      0.00         6\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00        17\n",
            "          83       0.00      0.00      0.00         4\n",
            "          84       0.00      0.00      0.00         7\n",
            "          85       0.00      0.00      0.00         8\n",
            "          86       1.00      0.16      0.27        19\n",
            "          87       0.00      0.00      0.00        10\n",
            "          88       0.00      0.00      0.00         3\n",
            "          89       0.00      0.00      0.00         7\n",
            "          90       0.00      0.00      0.00         3\n",
            "          91       0.00      0.00      0.00         4\n",
            "          92       0.00      0.00      0.00         8\n",
            "          93       0.00      0.00      0.00        10\n",
            "          94       0.50      0.11      0.18         9\n",
            "          95       0.00      0.00      0.00         9\n",
            "          96       1.00      0.14      0.25        14\n",
            "          97       0.12      0.25      0.17         4\n",
            "          98       0.03      0.04      0.03        26\n",
            "          99       0.00      0.00      0.00         4\n",
            "         100       1.00      0.07      0.13        14\n",
            "         101       0.00      0.00      0.00        12\n",
            "         102       0.00      0.00      0.00         6\n",
            "         103       0.20      0.07      0.10        29\n",
            "         104       0.00      0.00      0.00         9\n",
            "         105       0.00      0.00      0.00        14\n",
            "         106       0.00      0.00      0.00        16\n",
            "         107       0.00      0.00      0.00         9\n",
            "         108       0.04      0.07      0.05        15\n",
            "         109       0.00      0.00      0.00         9\n",
            "         110       0.00      0.00      0.00         6\n",
            "         111       0.25      0.14      0.18         7\n",
            "         112       0.00      0.00      0.00         9\n",
            "         113       0.10      0.25      0.14         4\n",
            "         114       0.00      0.00      0.00         6\n",
            "         115       0.00      0.00      0.00         6\n",
            "         116       1.00      0.10      0.17        21\n",
            "         117       0.21      0.28      0.24        18\n",
            "         118       0.33      0.33      0.33         3\n",
            "         119       0.10      0.14      0.12         7\n",
            "         120       0.00      0.00      0.00         9\n",
            "         121       0.00      0.00      0.00         7\n",
            "         122       0.00      0.00      0.00         9\n",
            "         123       0.00      0.00      0.00         6\n",
            "         124       0.00      0.00      0.00         3\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         6\n",
            "         127       0.00      0.00      0.00         6\n",
            "         128       0.00      0.00      0.00         9\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         3\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00        64\n",
            "         133       0.00      0.00      0.00         4\n",
            "         134       0.00      0.00      0.00         2\n",
            "         135       0.00      0.00      0.00         7\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         4\n",
            "         138       0.00      0.00      0.00         4\n",
            "         139       0.04      0.67      0.08        18\n",
            "         140       0.50      0.28      0.36        25\n",
            "         141       0.25      0.08      0.12        12\n",
            "         142       0.09      0.09      0.09        11\n",
            "         143       0.00      0.00      0.00        15\n",
            "         144       0.00      0.00      0.00         4\n",
            "         145       0.00      0.00      0.00         3\n",
            "         146       0.22      0.10      0.13        21\n",
            "         147       0.00      0.00      0.00         8\n",
            "         148       0.00      0.00      0.00         7\n",
            "         149       1.00      0.09      0.17        11\n",
            "         150       0.00      0.00      0.00        18\n",
            "         151       0.00      0.00      0.00        40\n",
            "         152       0.00      0.00      0.00         8\n",
            "         153       0.00      0.00      0.00         6\n",
            "         154       0.00      0.00      0.00         9\n",
            "         155       0.50      0.18      0.27        11\n",
            "         156       0.00      0.00      0.00         3\n",
            "         157       0.00      0.00      0.00        16\n",
            "         158       0.00      0.00      0.00         7\n",
            "         159       0.00      0.00      0.00         6\n",
            "         160       0.00      0.00      0.00         9\n",
            "         161       0.00      0.00      0.00        11\n",
            "         162       0.00      0.00      0.00        26\n",
            "         163       0.00      0.00      0.00        23\n",
            "         164       0.00      0.00      0.00         2\n",
            "         165       0.00      0.00      0.00         4\n",
            "         166       0.25      0.17      0.20         6\n",
            "         167       0.00      0.00      0.00         8\n",
            "         168       0.04      0.21      0.07        14\n",
            "         169       0.00      0.00      0.00        18\n",
            "         170       0.00      0.00      0.00        18\n",
            "         171       0.00      0.00      0.00         3\n",
            "         172       0.11      0.67      0.19        15\n",
            "         173       1.00      0.17      0.29         6\n",
            "         174       0.00      0.00      0.00         8\n",
            "         175       0.00      0.00      0.00         8\n",
            "         176       0.00      0.00      0.00         6\n",
            "         177       0.00      0.00      0.00        10\n",
            "         178       0.00      0.00      0.00         7\n",
            "         179       0.00      0.00      0.00        13\n",
            "         180       1.00      0.20      0.33         5\n",
            "         181       0.50      0.14      0.22         7\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.14      0.54      0.22        48\n",
            "         184       0.00      0.00      0.00        30\n",
            "         185       0.00      0.00      0.00        10\n",
            "         186       0.11      0.67      0.18         9\n",
            "         187       0.25      0.17      0.20        24\n",
            "         188       0.50      0.10      0.17        10\n",
            "         189       0.14      0.17      0.15         6\n",
            "         190       0.00      0.00      0.00         6\n",
            "         191       0.00      0.00      0.00         3\n",
            "         192       0.33      0.38      0.35         8\n",
            "         193       0.00      0.00      0.00        11\n",
            "         194       0.80      0.25      0.38        16\n",
            "         195       0.00      0.00      0.00         9\n",
            "         196       0.11      0.10      0.11        10\n",
            "         197       0.00      0.00      0.00         9\n",
            "         198       1.00      0.80      0.89         5\n",
            "         199       0.00      0.00      0.00         3\n",
            "         200       0.36      0.57      0.44        14\n",
            "         201       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.13      2396\n",
            "   macro avg       0.11      0.07      0.06      2396\n",
            "weighted avg       0.16      0.13      0.10      2396\n",
            "\n",
            "MobileNet:\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - acc: 0.2680 - loss: 21.1300\n",
            "Epoch 1: val_acc improved from -inf to 0.44616, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 151ms/step - acc: 0.2683 - loss: 21.1297 - val_acc: 0.4462 - val_loss: 19.5264\n",
            "Epoch 2/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - acc: 0.7998 - loss: 4.3389\n",
            "Epoch 2: val_acc improved from 0.44616 to 0.45534, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 144ms/step - acc: 0.7997 - loss: 4.3408 - val_acc: 0.4553 - val_loss: 22.9600\n",
            "Epoch 3/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - acc: 0.8463 - loss: 3.4365\n",
            "Epoch 3: val_acc improved from 0.45534 to 0.49207, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 144ms/step - acc: 0.8463 - loss: 3.4365 - val_acc: 0.4921 - val_loss: 21.7532\n",
            "Epoch 4/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - acc: 0.8872 - loss: 2.6750\n",
            "Epoch 4: val_acc did not improve from 0.49207\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 140ms/step - acc: 0.8872 - loss: 2.6757 - val_acc: 0.4750 - val_loss: 26.4367\n",
            "Epoch 5/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - acc: 0.8991 - loss: 2.3649\n",
            "Epoch 5: val_acc did not improve from 0.49207\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 139ms/step - acc: 0.8992 - loss: 2.3646 - val_acc: 0.4912 - val_loss: 26.7554\n",
            "Epoch 6/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - acc: 0.9139 - loss: 2.2581\n",
            "Epoch 6: val_acc did not improve from 0.49207\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 141ms/step - acc: 0.9139 - loss: 2.2574 - val_acc: 0.4866 - val_loss: 31.6309\n",
            "Epoch 7/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - acc: 0.9258 - loss: 1.7233\n",
            "Epoch 7: val_acc improved from 0.49207 to 0.49583, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 144ms/step - acc: 0.9258 - loss: 1.7238 - val_acc: 0.4958 - val_loss: 32.1237\n",
            "Epoch 8/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - acc: 0.9283 - loss: 1.7588\n",
            "Epoch 8: val_acc improved from 0.49583 to 0.49958, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 144ms/step - acc: 0.9283 - loss: 1.7596 - val_acc: 0.4996 - val_loss: 32.4525\n",
            "Epoch 9/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - acc: 0.9402 - loss: 1.5487\n",
            "Epoch 9: val_acc improved from 0.49958 to 0.50668, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 149ms/step - acc: 0.9403 - loss: 1.5480 - val_acc: 0.5067 - val_loss: 32.5304\n",
            "Epoch 10/20\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - acc: 0.9431 - loss: 1.4731\n",
            "Epoch 10: val_acc did not improve from 0.50668\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 145ms/step - acc: 0.9431 - loss: 1.4734 - val_acc: 0.4983 - val_loss: 35.7381\n",
            "Epoch 11/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - acc: 0.9459 - loss: 1.4891\n",
            "Epoch 11: val_acc did not improve from 0.50668\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 143ms/step - acc: 0.9459 - loss: 1.4902 - val_acc: 0.4971 - val_loss: 37.6963\n",
            "Epoch 12/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - acc: 0.9483 - loss: 1.5694\n",
            "Epoch 12: val_acc did not improve from 0.50668\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - acc: 0.9482 - loss: 1.5704 - val_acc: 0.5042 - val_loss: 38.9989\n",
            "Epoch 13/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - acc: 0.9483 - loss: 1.6022\n",
            "Epoch 13: val_acc did not improve from 0.50668\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - acc: 0.9483 - loss: 1.6010 - val_acc: 0.5004 - val_loss: 41.7408\n",
            "Epoch 14/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - acc: 0.9535 - loss: 1.4606\n",
            "Epoch 14: val_acc did not improve from 0.50668\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - acc: 0.9535 - loss: 1.4597 - val_acc: 0.5063 - val_loss: 39.7423\n",
            "Epoch 15/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - acc: 0.9567 - loss: 1.3731\n",
            "Epoch 15: val_acc improved from 0.50668 to 0.52254, saving model to best_model_mobilenetv2.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - acc: 0.9567 - loss: 1.3726 - val_acc: 0.5225 - val_loss: 41.8568\n",
            "Epoch 16/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - acc: 0.9703 - loss: 0.8814\n",
            "Epoch 16: val_acc did not improve from 0.52254\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - acc: 0.9703 - loss: 0.8828 - val_acc: 0.5163 - val_loss: 42.9360\n",
            "Epoch 17/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - acc: 0.9692 - loss: 1.0230\n",
            "Epoch 17: val_acc did not improve from 0.52254\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 143ms/step - acc: 0.9692 - loss: 1.0232 - val_acc: 0.4904 - val_loss: 47.1208\n",
            "Epoch 18/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - acc: 0.9631 - loss: 1.1014\n",
            "Epoch 18: val_acc did not improve from 0.52254\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 144ms/step - acc: 0.9631 - loss: 1.1024 - val_acc: 0.5054 - val_loss: 45.8707\n",
            "Epoch 19/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - acc: 0.9609 - loss: 1.4849\n",
            "Epoch 19: val_acc did not improve from 0.52254\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 142ms/step - acc: 0.9609 - loss: 1.4842 - val_acc: 0.5071 - val_loss: 49.3596\n",
            "Epoch 20/20\n",
            "\u001b[1m299/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - acc: 0.9589 - loss: 1.6064\n",
            "Epoch 20: val_acc did not improve from 0.52254\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - acc: 0.9589 - loss: 1.6048 - val_acc: 0.5063 - val_loss: 50.8174\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step\n",
            "\n",
            "Classification Report for mobilenetv2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.72      0.81        32\n",
            "           1       1.00      0.40      0.57         5\n",
            "           2       0.50      0.25      0.33        12\n",
            "           3       1.00      0.60      0.75         5\n",
            "           4       1.00      0.40      0.57         5\n",
            "           5       0.98      0.82      0.89        51\n",
            "           6       0.64      0.75      0.69        12\n",
            "           7       1.00      0.83      0.91         6\n",
            "           8       1.00      0.86      0.92         7\n",
            "           9       1.00      0.50      0.67         4\n",
            "          10       1.00      0.80      0.89         5\n",
            "          11       1.00      0.20      0.33         5\n",
            "          12       0.89      0.89      0.89         9\n",
            "          13       0.67      0.57      0.62         7\n",
            "          14       1.00      0.67      0.80         3\n",
            "          15       1.00      1.00      1.00         7\n",
            "          16       1.00      0.43      0.60         7\n",
            "          17       0.87      0.37      0.52        35\n",
            "          18       0.18      0.75      0.29        12\n",
            "          19       0.67      0.36      0.47        11\n",
            "          20       0.00      0.00      0.00         6\n",
            "          21       1.00      0.38      0.55         8\n",
            "          22       0.50      0.33      0.40         9\n",
            "          23       0.40      0.36      0.38        11\n",
            "          24       0.29      0.20      0.24        10\n",
            "          25       0.76      0.50      0.60        26\n",
            "          26       0.67      0.40      0.50        15\n",
            "          27       1.00      0.50      0.67        12\n",
            "          28       0.46      0.55      0.50        11\n",
            "          29       1.00      0.29      0.44         7\n",
            "          30       0.65      0.50      0.57        30\n",
            "          31       0.67      0.25      0.36         8\n",
            "          32       0.00      0.00      0.00         3\n",
            "          33       0.44      0.40      0.42        10\n",
            "          34       0.19      0.25      0.21        12\n",
            "          35       1.00      0.57      0.73         7\n",
            "          36       0.55      0.69      0.61        35\n",
            "          37       0.15      0.40      0.22         5\n",
            "          38       0.50      0.78      0.61        37\n",
            "          39       0.43      0.55      0.48        33\n",
            "          40       1.00      0.64      0.78        14\n",
            "          41       0.50      0.17      0.25         6\n",
            "          42       0.44      0.50      0.47        24\n",
            "          43       0.00      0.00      0.00         3\n",
            "          44       1.00      0.29      0.44         7\n",
            "          45       0.56      0.66      0.61        53\n",
            "          46       0.38      0.71      0.50         7\n",
            "          47       0.27      0.61      0.37        18\n",
            "          48       0.45      0.68      0.54        66\n",
            "          49       1.00      0.11      0.20         9\n",
            "          50       0.25      0.25      0.25         4\n",
            "          51       0.77      1.00      0.87        10\n",
            "          52       0.88      0.88      0.88         8\n",
            "          53       0.44      0.57      0.50         7\n",
            "          54       1.00      0.75      0.86         4\n",
            "          55       1.00      0.57      0.73         7\n",
            "          56       0.00      0.00      0.00         3\n",
            "          57       0.00      0.00      0.00         4\n",
            "          58       0.75      0.62      0.68        24\n",
            "          59       0.61      0.52      0.56        21\n",
            "          60       0.43      0.75      0.55         4\n",
            "          61       0.46      0.60      0.52        10\n",
            "          62       1.00      0.75      0.86         4\n",
            "          63       0.50      0.22      0.31        18\n",
            "          64       1.00      0.08      0.14        13\n",
            "          65       0.60      0.50      0.55         6\n",
            "          66       0.50      0.50      0.50         4\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.66      0.82      0.73        62\n",
            "          69       0.12      0.33      0.18         3\n",
            "          70       0.36      0.79      0.49        28\n",
            "          71       0.75      0.38      0.50         8\n",
            "          72       0.25      0.33      0.29         3\n",
            "          73       0.67      0.22      0.33         9\n",
            "          74       0.71      0.65      0.68        26\n",
            "          75       1.00      0.50      0.67         6\n",
            "          76       0.63      0.63      0.63        27\n",
            "          77       0.71      0.59      0.65        17\n",
            "          78       1.00      0.33      0.50         6\n",
            "          79       0.08      0.90      0.14        10\n",
            "          80       0.71      0.83      0.77         6\n",
            "          81       0.67      0.40      0.50         5\n",
            "          82       1.00      0.29      0.45        17\n",
            "          83       0.25      0.50      0.33         4\n",
            "          84       0.18      0.29      0.22         7\n",
            "          85       1.00      0.25      0.40         8\n",
            "          86       0.75      0.16      0.26        19\n",
            "          87       1.00      0.70      0.82        10\n",
            "          88       0.00      0.00      0.00         3\n",
            "          89       0.00      0.00      0.00         7\n",
            "          90       0.60      1.00      0.75         3\n",
            "          91       0.06      0.50      0.11         4\n",
            "          92       0.00      0.00      0.00         8\n",
            "          93       0.31      0.50      0.38        10\n",
            "          94       1.00      0.44      0.62         9\n",
            "          95       0.80      0.44      0.57         9\n",
            "          96       0.88      0.50      0.64        14\n",
            "          97       0.00      0.00      0.00         4\n",
            "          98       0.53      0.65      0.59        26\n",
            "          99       1.00      0.50      0.67         4\n",
            "         100       0.90      0.64      0.75        14\n",
            "         101       0.38      0.42      0.40        12\n",
            "         102       0.20      0.17      0.18         6\n",
            "         103       0.65      0.38      0.48        29\n",
            "         104       0.44      0.78      0.56         9\n",
            "         105       0.53      0.57      0.55        14\n",
            "         106       0.50      0.31      0.38        16\n",
            "         107       1.00      0.22      0.36         9\n",
            "         108       0.67      0.40      0.50        15\n",
            "         109       1.00      0.78      0.88         9\n",
            "         110       1.00      0.50      0.67         6\n",
            "         111       0.67      0.29      0.40         7\n",
            "         112       1.00      0.22      0.36         9\n",
            "         113       0.75      0.75      0.75         4\n",
            "         114       0.57      0.67      0.62         6\n",
            "         115       1.00      0.83      0.91         6\n",
            "         116       0.41      0.43      0.42        21\n",
            "         117       0.43      0.83      0.57        18\n",
            "         118       0.29      0.67      0.40         3\n",
            "         119       1.00      0.71      0.83         7\n",
            "         120       0.29      0.44      0.35         9\n",
            "         121       0.67      0.86      0.75         7\n",
            "         122       0.50      0.11      0.18         9\n",
            "         123       0.29      0.33      0.31         6\n",
            "         124       1.00      0.67      0.80         3\n",
            "         125       0.12      0.20      0.15         5\n",
            "         126       0.00      0.00      0.00         6\n",
            "         127       1.00      0.33      0.50         6\n",
            "         128       0.33      0.11      0.17         9\n",
            "         129       0.75      0.60      0.67         5\n",
            "         130       0.33      0.33      0.33         3\n",
            "         131       0.50      0.60      0.55         5\n",
            "         132       0.52      0.80      0.63        64\n",
            "         133       1.00      0.75      0.86         4\n",
            "         134       1.00      0.50      0.67         2\n",
            "         135       0.33      0.14      0.20         7\n",
            "         136       0.33      0.20      0.25         5\n",
            "         137       0.43      0.75      0.55         4\n",
            "         138       0.00      0.00      0.00         4\n",
            "         139       0.38      0.72      0.50        18\n",
            "         140       0.55      0.24      0.33        25\n",
            "         141       0.23      0.92      0.37        12\n",
            "         142       0.67      0.73      0.70        11\n",
            "         143       1.00      0.13      0.24        15\n",
            "         144       0.50      0.50      0.50         4\n",
            "         145       0.00      0.00      0.00         3\n",
            "         146       0.24      0.33      0.28        21\n",
            "         147       1.00      0.88      0.93         8\n",
            "         148       0.12      0.29      0.17         7\n",
            "         149       1.00      0.55      0.71        11\n",
            "         150       0.25      0.17      0.20        18\n",
            "         151       0.69      0.28      0.39        40\n",
            "         152       0.67      0.25      0.36         8\n",
            "         153       1.00      0.17      0.29         6\n",
            "         154       0.89      0.89      0.89         9\n",
            "         155       0.25      0.64      0.36        11\n",
            "         156       1.00      1.00      1.00         3\n",
            "         157       1.00      0.25      0.40        16\n",
            "         158       1.00      0.14      0.25         7\n",
            "         159       1.00      0.17      0.29         6\n",
            "         160       1.00      0.22      0.36         9\n",
            "         161       0.33      0.27      0.30        11\n",
            "         162       0.64      0.54      0.58        26\n",
            "         163       0.67      0.17      0.28        23\n",
            "         164       0.00      0.00      0.00         2\n",
            "         165       0.50      0.25      0.33         4\n",
            "         166       0.25      0.67      0.36         6\n",
            "         167       1.00      0.38      0.55         8\n",
            "         168       0.78      0.50      0.61        14\n",
            "         169       0.79      0.83      0.81        18\n",
            "         170       1.00      0.39      0.56        18\n",
            "         171       0.14      0.33      0.20         3\n",
            "         172       1.00      0.73      0.85        15\n",
            "         173       0.83      0.83      0.83         6\n",
            "         174       0.60      0.38      0.46         8\n",
            "         175       0.44      0.50      0.47         8\n",
            "         176       1.00      0.17      0.29         6\n",
            "         177       1.00      0.40      0.57        10\n",
            "         178       1.00      1.00      1.00         7\n",
            "         179       0.40      0.31      0.35        13\n",
            "         180       0.33      0.60      0.43         5\n",
            "         181       1.00      0.43      0.60         7\n",
            "         182       1.00      0.40      0.57         5\n",
            "         183       0.65      0.77      0.70        48\n",
            "         184       0.74      0.47      0.57        30\n",
            "         185       0.38      0.50      0.43        10\n",
            "         186       0.67      0.22      0.33         9\n",
            "         187       0.64      0.38      0.47        24\n",
            "         188       0.50      0.30      0.38        10\n",
            "         189       0.75      0.50      0.60         6\n",
            "         190       0.00      0.00      0.00         6\n",
            "         191       0.27      1.00      0.43         3\n",
            "         192       1.00      0.62      0.77         8\n",
            "         193       0.36      0.36      0.36        11\n",
            "         194       0.75      0.75      0.75        16\n",
            "         195       0.33      0.44      0.38         9\n",
            "         196       0.89      0.80      0.84        10\n",
            "         197       0.56      0.56      0.56         9\n",
            "         198       1.00      1.00      1.00         5\n",
            "         199       0.29      0.67      0.40         3\n",
            "         200       1.00      0.64      0.78        14\n",
            "         201       1.00      0.55      0.71        11\n",
            "\n",
            "    accuracy                           0.52      2396\n",
            "   macro avg       0.62      0.47      0.49      2396\n",
            "weighted avg       0.63      0.52      0.53      2396\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# we dicided to run for 20 epochs as otherwise the run might be two long, we hope that is enough to already see patterns in the scores\n",
        "epochs = 20\n",
        "\n",
        "model_vgg16 = make_model_vgg16(input_shape=image_size + (3,), num_classes=202)\n",
        "model_resnet50 = make_model_resnet50(input_shape=image_size + (3,), num_classes=202)\n",
        "model_mobilenet = make_model_mobilenetv2(input_shape=image_size + (3,), num_classes=202)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"VGG16:\")\n",
        "result_vgg16 = train_and_evaluate_model(\n",
        "    model=model_vgg16,\n",
        "    model_name=\"vgg16\",\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Resnet50:\")\n",
        "result_resnet50 = train_and_evaluate_model(\n",
        "    model=model_resnet50,\n",
        "    model_name=\"resnet50\",\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "\n",
        "print(\"MobileNet:\")\n",
        "result_mobilenet = train_and_evaluate_model(\n",
        "    model=model_mobilenet,\n",
        "    model_name=\"mobilenetv2\",\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
